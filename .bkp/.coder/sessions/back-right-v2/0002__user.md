Neste projeto, o componente de Sidebar consome duas rotas do arquivo prompts.py, list-prompts e create-prompts, na logica do código, a Sidebar consome a API de List-prompts e retorna uma lista dos prompts, o meu desejo é de que o usuário selecione os prompts desejados e clique no botão de Save Select, quando clica no botão de Save Select ele envia os dados para a api de create-prompt, a API de Create-Prompt, executa apenas a criação de 1 prompt logo quando o usuário seleciona mais de 1 prompt existente, eu quero que estes prompts sejam concatenados e para isso eu irei utilizar outra rota de LLM que irá receber esses prompts e criar um novo com todo esse contexto. Quero que você implemente a rota de LLM dentro da rota de /create-prompts e envie o output dessa rota para a rota post dee prompt-templates dentro do código, segue o codigo:

async def send_message(request: ChatRequest):
    url = "https://dev.flow.ciandt.com/ai-orchestration-api/v1/openai/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "flowModel":"gpt-4o",
        "flowChannel":"chat-with-docs",
        "flowAgent":"CoderChallenge",
        "Authorization": f"Bearer {os.getenv('AUTH_TOKEN')}"  # Use token from environment variable
    }
    payload = request.dict()

    async with httpx.AsyncClient() as client:
        response = await client.post(url, headers=headers, json=payload)
        if response.status_code != 200:
            raise HTTPException(status_code=response.status_code, detail=response.text)
        return response.json()